{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johannesmichael/AMLD/blob/main/02_Intro_to_Q_learning_and_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HYoMMvnUyzd"
      },
      "source": [
        "Setup\n",
        "---\n",
        "\n",
        "Make sure to select `GPU` under Runtime > Change runtime type > Hardware accelerator!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ERhn3J-HYsk"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Checks that the Runtime is correct\n",
        "if 'google.colab' in sys.modules:\n",
        "    !nvidia-smi | grep -q 'failed' && echo \"STOP! You are using a runtime without a GPU. Change the runtime type before going further!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOC5qCkgUyzg"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Setup for use in Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Clone GitHub repository\n",
        "    !git clone https://github.com/AIcrowd/droneRL-workshop\n",
        "\n",
        "    # Install packages via pip\n",
        "    !pip install -r \"droneRL-workshop/colab-requirements.txt\"\n",
        "\n",
        "    # Restart Runtime so everything takes effect\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "    # Your Runtime will crash after this - this is normal!\n",
        "    # Resume from next cell after it restarted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QB-SDGR5P9y"
      },
      "source": [
        "%cd droneRL-workshop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkocSoiD5RaW"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLr2aS4mUyzx"
      },
      "source": [
        "Intro to Q-Learning (compass Q-table)\n",
        "---\n",
        "\n",
        "You can find a Q-learning implementation in `agents/`\n",
        "\n",
        "```\n",
        "agents/\n",
        "├── curiosity.py\n",
        "├── dqn.py\n",
        "├── logging.py\n",
        "├── qlearning.py    <-- Q-learning agent\n",
        "└── random.py\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbK6d8NUUyzy"
      },
      "source": [
        "from env.env import DeliveryDrones\n",
        "from env.wrappers import CompassQTable, CompassChargeQTable, LidarCompassQTable, LidarCompassChargeQTable\n",
        "\n",
        "# Environment without Skyscrapers + discharge\n",
        "env = CompassQTable(DeliveryDrones())\n",
        "env.env_params.update({'n_drones': 3, 'skyscrapers_factor': 0, 'stations_factor': 0,  'discharge': 0})\n",
        "states = env.reset()\n",
        "\n",
        "print('Observation space:', env.observation_space)\n",
        "print('Initial state:', {drone_index: env.format_state(state) for drone_index, state in states.items()})\n",
        "Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGfDx6f2Uyz1"
      },
      "source": [
        "from agents.random import RandomAgent\n",
        "from agents.qlearning import QLearningAgent\n",
        "\n",
        "# Create random agents\n",
        "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
        "\n",
        "# Create one Q-learning agent\n",
        "agents[0] = QLearningAgent(\n",
        "    env,\n",
        "    gamma=0.95, # Discount factor\n",
        "    alpha=0.1, # Learning rate\n",
        "    # Exploration rate\n",
        "    epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01\n",
        ")\n",
        "\n",
        "agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s813-eptUyz7"
      },
      "source": [
        "from helpers.rl_helpers import MultiAgentTrainer, plot_rolling_rewards, test_agents, plot_cumulative_rewards\n",
        "\n",
        "# Train agents\n",
        "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
        "trainer.train(5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RVKwutFUyz-"
      },
      "source": [
        "plot_rolling_rewards(trainer.rewards_log, drones_labels={0: 'Q-learning'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzOkvRWeUy0A"
      },
      "source": [
        "from env.env import DeliveryDrones\n",
        "\n",
        "agents[0].get_qtable()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIEipSHPUy0E"
      },
      "source": [
        "plt.plot(agents[0].gamma**np.arange(100))\n",
        "plt.title('Discount factor: {}'.format(agents[0].gamma))\n",
        "plt.xlabel('Number of steps')\n",
        "plt.ylabel('Discount')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm2xVZruUy0H"
      },
      "source": [
        "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
        "plot_cumulative_rewards(rewards_log, drones_labels={0: 'Q-learning'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GONoe76Uy0K"
      },
      "source": [
        "from helpers.rl_helpers import render_video, ColabVideo\n",
        "\n",
        "path = os.path.join('output', 'videos', 'ql-compass.mp4')\n",
        "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
        "ColabVideo(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDqhulIqUy0N"
      },
      "source": [
        "Scaling Q-learning (compass + lidar Q-table)\n",
        "---\n",
        "\n",
        "Let's see how Q-learning scales to larger observation spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izDZjq0dUy0O"
      },
      "source": [
        "# Environment with skyscrapers but without discharge\n",
        "env = LidarCompassQTable(DeliveryDrones())\n",
        "env.env_params.update({'n_drones': 3, 'skyscrapers_factor': 3, 'stations_factor': 0, 'discharge': 0})\n",
        "states = env.reset()\n",
        "\n",
        "print('Observation space:', env.observation_space)\n",
        "print('Sample state:', {drone_index: env.format_state(state) for drone_index, state in states.items()})\n",
        "Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L79FTJ_XUy0R"
      },
      "source": [
        "# Create the agents\n",
        "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
        "agents[0] = QLearningAgent(\n",
        "    env,\n",
        "    gamma=0.95, # Discount factor\n",
        "    alpha=0.1, # Learning rate\n",
        "    # Exploration rate\n",
        "    epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01\n",
        ")\n",
        "agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRaa4VTaUy0T"
      },
      "source": [
        "# Train agents\n",
        "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
        "trainer.train(5000)\n",
        "plot_rolling_rewards(trainer.rewards_log, drones_labels={0: 'Q-learning'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yGIpaa0Uy0X"
      },
      "source": [
        "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
        "plot_cumulative_rewards(rewards_log, drones_labels={0: 'Q-learning'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLNOPGY-Uy0a"
      },
      "source": [
        "path = os.path.join('output', 'videos', 'ql-compass-lidar-1st-try.mp4')\n",
        "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
        "ColabVideo(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uZvpptIUy0d"
      },
      "source": [
        "Issues with Q-learning\n",
        "---\n",
        "\n",
        "Two issues here\n",
        "\n",
        "* Sparse reward: pickup rate is around 1%\n",
        "* No generalization: need to explore entire space!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nYMS72eUy0d"
      },
      "source": [
        "q_table = agents[0].get_qtable()\n",
        "print('Q-table:', q_table.shape)\n",
        "q_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaRv4FJBUy0g"
      },
      "source": [
        "plt.plot(agents[0].epsilons)\n",
        "plt.xlabel('Number of episodes')\n",
        "plt.ylabel('Exploration rate (epsilon)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP0BfD6ZUy0i"
      },
      "source": [
        "Possible solutions\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXvcNBJHUy0j"
      },
      "source": [
        "from helpers.rl_helpers import set_seed\n",
        "\n",
        "# (1/2) Sparse rewards: Create an intermediate \"pickup\" reward to help\n",
        "env.env_params.update({\n",
        "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1.0,\n",
        "    'skyscrapers_factor': 3, 'stations_factor': 0, 'discharge': 0})\n",
        "states = env.reset()\n",
        "\n",
        "# (2/2) Train longer...\n",
        "agents[0].epsilon = 1.0\n",
        "agents[0].epsilon_decay = 0.999\n",
        "\n",
        "set_seed(env, seed=0) # Make things deterministic\n",
        "trainer.train(30000)\n",
        "\n",
        "plot_rolling_rewards(\n",
        "    trainer.rewards_log,\n",
        "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
        "    drones_labels={0: 'Q-learning'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9-Di0UdUy0n"
      },
      "source": [
        "plt.plot(agents[0].epsilons)\n",
        "plt.xlabel('Number of episodes')\n",
        "plt.ylabel('Exploration rate (epsilon)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGn--9h0Uy0q"
      },
      "source": [
        "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
        "plot_cumulative_rewards(\n",
        "    rewards_log,\n",
        "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
        "    drones_labels={0: 'Q-learning'}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcSe5L8iUy0t"
      },
      "source": [
        "Overfitting issues: try with different seeds\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQA2LqdOUy0t"
      },
      "source": [
        "rewards_log = test_agents(env, agents, n_steps=1000, seed=1)\n",
        "plot_cumulative_rewards(\n",
        "    rewards_log,\n",
        "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
        "    drones_labels={0: 'Q-learning'}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaybsZareIIT"
      },
      "source": [
        "The agent only learned to act in a specific environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUP4wGm4Uy00"
      },
      "source": [
        "Q-learning limitations: discrete Q-table!\n",
        "---\n",
        "\n",
        "Let's try Q-learning with the full environment: skyscrapers + charge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2bo-ZfoUy01"
      },
      "source": [
        "env = LidarCompassChargeQTable(DeliveryDrones())\n",
        "env.env_params.update({\n",
        "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1,\n",
        "    'discharge': 10, 'charge': 20, 'charge_reward': -0.1\n",
        "})\n",
        "states = env.reset()\n",
        "\n",
        "print('Observation space:', env.observation_space)\n",
        "print('Sample state:', env.format_state(states[0]))\n",
        "Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ9nORBUUy03"
      },
      "source": [
        "# Create the agents\n",
        "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
        "agents[0] = QLearningAgent(\n",
        "    env, gamma=0.95, alpha=0.1,\n",
        "    epsilon_start=1, epsilon_decay=0.999, epsilon_end=0.01\n",
        ")\n",
        "\n",
        "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
        "trainer.train(35000)\n",
        "plot_rolling_rewards(trainer.rewards_log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFLqZBmMUy06"
      },
      "source": [
        "q_table = agents[0].get_qtable()\n",
        "print('Q-table:', q_table.shape)\n",
        "q_table.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT-1YyNQUy09"
      },
      "source": [
        "Don't forget to test with different seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvCnWPFSUy0-"
      },
      "source": [
        "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
        "plot_cumulative_rewards(\n",
        "    rewards_log,\n",
        "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]},\n",
        "    drones_labels={0: 'Q-learning'}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeFqgVd1Uy1B"
      },
      "source": [
        "Note that for now, we are only training our agent in a single environment: the charging points, skyscrapers etc are always at the same position.\n",
        "\n",
        "But during evaluation, the environment won't be the same!\n",
        "\n",
        "Resetting the environment every X steps would help, but won't solve the important limitations with Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jg68dr9Uy1B"
      },
      "source": [
        "rewards_log = test_agents(env, agents, n_steps=1000, seed=1)\n",
        "plot_cumulative_rewards(\n",
        "    rewards_log,\n",
        "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]},\n",
        "    drones_labels={0: 'Q-learning'}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxc09jsyUy1E"
      },
      "source": [
        "First tests with deep Q-learning (DQN)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-gdJaRNUy1F"
      },
      "source": [
        "from agents.dqn import DQNAgent, DenseQNetworkFactory\n",
        "\n",
        "# Create environment\n",
        "env = LidarCompassChargeQTable(DeliveryDrones())\n",
        "env.env_params.update({\n",
        "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1\n",
        "})\n",
        "states = env.reset()\n",
        "\n",
        "# Create the agents\n",
        "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
        "agents[0] = DQNAgent(\n",
        "    env, DenseQNetworkFactory(env, hidden_layers=[256, 256]),\n",
        "    gamma=0.95, epsilon_start=1, epsilon_decay=0.999, epsilon_end=0.01,\n",
        "    memory_size=10000, batch_size=64, target_update_interval=5\n",
        ")\n",
        "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
        "agents[0].qnetwork"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHRN104hUy1J"
      },
      "source": [
        "# Train the agents\n",
        "trainer.train(25000)\n",
        "plot_rolling_rewards(\n",
        "    trainer.rewards_log, drones_labels={0: 'DQN'},\n",
        "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xPZdEipUy1L"
      },
      "source": [
        "plt.plot(agents[0].epsilons)\n",
        "plt.xlabel('Number of episodes')\n",
        "plt.ylabel('Exploration rate (epsilon)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRcXMtLgUy1Q"
      },
      "source": [
        "Try with different seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGKesCiKUy1S"
      },
      "source": [
        "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
        "plot_cumulative_rewards(\n",
        "    rewards_log, drones_labels={0: 'DQN'},\n",
        "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enf8PZEZUy1U"
      },
      "source": [
        "# Inspect replay memory buffer\n",
        "agents[0].inspect_memory(top_n=10, max_col=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omlSDb-9Uy1W"
      },
      "source": [
        "Take a moment to play with the different parameters: `memory_size`, `batch_size`, `target_update_interval`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BklaA8CpUy1X"
      },
      "source": [
        "path = os.path.join('output', 'videos', 'dqn-compass-lidar-charge.mp4')\n",
        "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
        "ColabVideo(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKf7xskwUy1Y"
      },
      "source": [
        "DQN and WindowedGrid\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZhqD-kkUy1Z"
      },
      "source": [
        "from env.wrappers import WindowedGridView\n",
        "from agents.dqn import ConvQNetworkFactory\n",
        "\n",
        "# Create environment\n",
        "env = WindowedGridView(DeliveryDrones(), radius=3)\n",
        "env.env_params.update({\n",
        "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1\n",
        "})\n",
        "states = env.reset()\n",
        "\n",
        "# Create the agents\n",
        "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
        "agents[0] = my_agent = DQNAgent(\n",
        "    env, ConvQNetworkFactory(env, conv_layers=[\n",
        "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
        "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
        "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
        "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
        "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
        "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
        "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
        "    ], dense_layers=[1024, 256]),\n",
        "    gamma=0.95, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01,\n",
        "    memory_size=10000, batch_size=64, target_update_interval=5\n",
        ")\n",
        "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
        "agents[0].qnetwork"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fiqt2dVBUy1b"
      },
      "source": [
        "# Train the agents\n",
        "for run in range(10):\n",
        "  trainer.train(2500)\n",
        "  plot_rolling_rewards(\n",
        "      trainer.rewards_log, drones_labels={0: 'DQN'},\n",
        "      events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6ygh8puJEvW"
      },
      "source": [
        "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
        "plot_cumulative_rewards(\n",
        "    rewards_log, drones_labels={0: 'DQN'},\n",
        "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})\n",
        "\n",
        "# Print final evaluation scores\n",
        "print('Final scores:')\n",
        "for idx, score in enumerate(np.sum(list(rewards_log.values()), axis=1)):\n",
        "    print(\"Agent {}: {}\".format(idx, score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbjGOe3_Uy1e"
      },
      "source": [
        "path = os.path.join('output', 'videos', 'dqn-windowed.mp4')\n",
        "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
        "ColabVideo(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoiOo4bhUy1i"
      },
      "source": [
        "## Submit to AIcrowd! 🚀\n",
        "\n",
        "> https://www.aicrowd.com/challenges/dronerl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SATrBaezUy1j"
      },
      "source": [
        "path = os.path.join('output', 'agents', 'dqn-agent.pt')\n",
        "agents[0].save(path)\n",
        "# agents[0].load(path) # Later, load the qnetwork!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}